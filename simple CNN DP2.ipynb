{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "enclosed-dominant",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D,Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sophisticated-disorder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>569587686496825344</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KristenReenders</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 12:01:01 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>569587371693355008</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>itsropes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:46 -0800</td>\n",
       "      <td>Texas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>569587242672398336</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sanyabun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:15 -0800</td>\n",
       "      <td>Nigeria,lagos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>569587188687634433</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>0.6659</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SraJackson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:02 -0800</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>569587140490866689</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>daviddtwu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:58:51 -0800</td>\n",
       "      <td>dallas, TX</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0      570306133677760513           neutral                        1.0000   \n",
       "1      570301130888122368          positive                        0.3486   \n",
       "2      570301083672813571           neutral                        0.6837   \n",
       "3      570301031407624196          negative                        1.0000   \n",
       "4      570300817074462722          negative                        1.0000   \n",
       "...                   ...               ...                           ...   \n",
       "14635  569587686496825344          positive                        0.3487   \n",
       "14636  569587371693355008          negative                        1.0000   \n",
       "14637  569587242672398336           neutral                        1.0000   \n",
       "14638  569587188687634433          negative                        1.0000   \n",
       "14639  569587140490866689           neutral                        0.6771   \n",
       "\n",
       "               negativereason  negativereason_confidence         airline  \\\n",
       "0                         NaN                        NaN  Virgin America   \n",
       "1                         NaN                     0.0000  Virgin America   \n",
       "2                         NaN                        NaN  Virgin America   \n",
       "3                  Bad Flight                     0.7033  Virgin America   \n",
       "4                  Can't Tell                     1.0000  Virgin America   \n",
       "...                       ...                        ...             ...   \n",
       "14635                     NaN                     0.0000        American   \n",
       "14636  Customer Service Issue                     1.0000        American   \n",
       "14637                     NaN                        NaN        American   \n",
       "14638  Customer Service Issue                     0.6659        American   \n",
       "14639                     NaN                     0.0000        American   \n",
       "\n",
       "      airline_sentiment_gold             name negativereason_gold  \\\n",
       "0                        NaN          cairdin                 NaN   \n",
       "1                        NaN         jnardino                 NaN   \n",
       "2                        NaN       yvonnalynn                 NaN   \n",
       "3                        NaN         jnardino                 NaN   \n",
       "4                        NaN         jnardino                 NaN   \n",
       "...                      ...              ...                 ...   \n",
       "14635                    NaN  KristenReenders                 NaN   \n",
       "14636                    NaN         itsropes                 NaN   \n",
       "14637                    NaN         sanyabun                 NaN   \n",
       "14638                    NaN       SraJackson                 NaN   \n",
       "14639                    NaN        daviddtwu                 NaN   \n",
       "\n",
       "       retweet_count                                               text  \\\n",
       "0                  0                @VirginAmerica What @dhepburn said.   \n",
       "1                  0  @VirginAmerica plus you've added commercials t...   \n",
       "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
       "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
       "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
       "...              ...                                                ...   \n",
       "14635              0  @AmericanAir thank you we got on a different f...   \n",
       "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
       "14637              0  @AmericanAir Please bring American Airlines to...   \n",
       "14638              0  @AmericanAir you have my money, you change my ...   \n",
       "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
       "\n",
       "      tweet_coord              tweet_created tweet_location  \\\n",
       "0             NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "...           ...                        ...            ...   \n",
       "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
       "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
       "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
       "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
       "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
       "\n",
       "                    user_timezone  \n",
       "0      Eastern Time (US & Canada)  \n",
       "1      Pacific Time (US & Canada)  \n",
       "2      Central Time (US & Canada)  \n",
       "3      Pacific Time (US & Canada)  \n",
       "4      Pacific Time (US & Canada)  \n",
       "...                           ...  \n",
       "14635                         NaN  \n",
       "14636                         NaN  \n",
       "14637                         NaN  \n",
       "14638  Eastern Time (US & Canada)  \n",
       "14639                         NaN  \n",
       "\n",
       "[14640 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the Tweets Data set \n",
    "data = pd.read_csv('....\\Tweets.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "painful-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text','airline_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eight-sample",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-eb60cc312d1e>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'] = data['text'].apply(lambda x: x.lower())\n"
     ]
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "#data['text'] = data['text'].apply((lambda x: re.sub('@\\w+','',x)))\n",
    "#data['text'] = data['text'].apply((lambda x: re.sub('http\\S+','',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "foster-pharmaceutical",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-5a5269ec28b9>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n"
     ]
    }
   ],
   "source": [
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "improving-geneva",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4726\n",
      "18356\n",
      "6198\n"
     ]
    }
   ],
   "source": [
    "# Printing the size of the Positive, Neutral and Negatives data points in the Tweets data set\n",
    "print(data[ data['airline_sentiment'] == 'positive'].size)\n",
    "print(data[ data['airline_sentiment'] == 'negative'].size)\n",
    "print(data[ data['airline_sentiment'] == 'neutral'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "contemporary-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "indie-prayer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>virginamerica what dhepburn said</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>virginamerica plus youve added commercials to ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>virginamerica i didnt today must mean i need t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>virginamerica its really aggressive to blast o...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>virginamerica and its a really big bad thing a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>americanair thank you we got on a different fl...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>americanair leaving over 20 minutes late fligh...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>americanair please bring american airlines to ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>americanair you have my money you change my fl...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>americanair we have 8 ppl so we need 2 know ho...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text airline_sentiment\n",
       "0                       virginamerica what dhepburn said           neutral\n",
       "1      virginamerica plus youve added commercials to ...          positive\n",
       "2      virginamerica i didnt today must mean i need t...           neutral\n",
       "3      virginamerica its really aggressive to blast o...          negative\n",
       "4      virginamerica and its a really big bad thing a...          negative\n",
       "...                                                  ...               ...\n",
       "14635  americanair thank you we got on a different fl...          positive\n",
       "14636  americanair leaving over 20 minutes late fligh...          negative\n",
       "14637  americanair please bring american airlines to ...           neutral\n",
       "14638  americanair you have my money you change my fl...          negative\n",
       "14639  americanair we have 8 ppl so we need 2 know ho...           neutral\n",
       "\n",
       "[14640 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sorted-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokeninizing the data and padding them\n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acute-briefs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9808, 32) (9808, 3)\n",
      "(4832, 32) (4832, 3)\n"
     ]
    }
   ],
   "source": [
    "# Splittting of the data set into training and testing sets\n",
    "Y = pd.get_dummies(data['airline_sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vital-short",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 128)           256000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 32, 32)            20512     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 281,123\n",
      "Trainable params: 281,123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "26/26 [==============================] - 5s 126ms/step - loss: 1.0142 - accuracy: 0.5382 - val_loss: 0.8701 - val_accuracy: 0.6305\n",
      "Epoch 2/20\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 0.8495 - accuracy: 0.6184 - val_loss: 0.7535 - val_accuracy: 0.6503\n",
      "Epoch 3/20\n",
      "26/26 [==============================] - 2s 58ms/step - loss: 0.6684 - accuracy: 0.6964 - val_loss: 0.6101 - val_accuracy: 0.7584\n",
      "Epoch 4/20\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 0.4728 - accuracy: 0.8186 - val_loss: 0.5698 - val_accuracy: 0.7748\n",
      "Epoch 5/20\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 0.3507 - accuracy: 0.8763 - val_loss: 0.5601 - val_accuracy: 0.7844\n",
      "Epoch 6/20\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.2571 - accuracy: 0.9116 - val_loss: 0.5829 - val_accuracy: 0.7859\n",
      "Epoch 7/20\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.1894 - accuracy: 0.9412 - val_loss: 0.6203 - val_accuracy: 0.7875\n",
      "Epoch 8/20\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.1469 - accuracy: 0.9607 - val_loss: 0.6664 - val_accuracy: 0.7822\n",
      "Epoch 9/20\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.1035 - accuracy: 0.9771 - val_loss: 0.7181 - val_accuracy: 0.7828\n",
      "Epoch 10/20\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.0801 - accuracy: 0.9828 - val_loss: 0.7665 - val_accuracy: 0.7813\n",
      "Epoch 11/20\n",
      "26/26 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.98 - 1s 54ms/step - loss: 0.0588 - accuracy: 0.9885 - val_loss: 0.8086 - val_accuracy: 0.7794\n",
      "Epoch 12/20\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.0436 - accuracy: 0.9902 - val_loss: 0.8604 - val_accuracy: 0.7754\n",
      "Epoch 13/20\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.0371 - accuracy: 0.9913 - val_loss: 0.9012 - val_accuracy: 0.7776\n",
      "Epoch 14/20\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.0326 - accuracy: 0.9932 - val_loss: 0.9319 - val_accuracy: 0.7739\n",
      "Epoch 15/20\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.0318 - accuracy: 0.9926 - val_loss: 0.9709 - val_accuracy: 0.7763\n",
      "Epoch 16/20\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.0251 - accuracy: 0.9929 - val_loss: 0.9989 - val_accuracy: 0.7745\n",
      "Epoch 17/20\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.0224 - accuracy: 0.9937 - val_loss: 1.0362 - val_accuracy: 0.7773\n",
      "Epoch 18/20\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.0187 - accuracy: 0.9955 - val_loss: 1.0580 - val_accuracy: 0.7773\n",
      "Epoch 19/20\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 0.0251 - accuracy: 0.9922 - val_loss: 1.0650 - val_accuracy: 0.7723\n",
      "Epoch 20/20\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 0.0172 - accuracy: 0.9939 - val_loss: 1.1168 - val_accuracy: 0.7766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x259c53c8850>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CNN\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "embed_dim = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(Conv1D(filters=32, kernel_size=5, activation='relu', padding='same'))\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128,activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train,Y_train,epochs = 20,batch_size=256,validation_split = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "excess-hampshire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2772  360  152]\n",
      " [ 224  525  101]\n",
      " [  89   99  510]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87      3284\n",
      "           1       0.53      0.62      0.57       850\n",
      "           2       0.67      0.73      0.70       698\n",
      "\n",
      "    accuracy                           0.79      4832\n",
      "   macro avg       0.70      0.73      0.71      4832\n",
      "weighted avg       0.80      0.79      0.79      4832\n",
      "\n",
      "0.7878725165562914\n"
     ]
    }
   ],
   "source": [
    "# A Function to print Confusion Matrix, Classification Report and Accuracy Score\n",
    "def results(labels, pred):\n",
    "    print(confusion_matrix(labels,pred))  \n",
    "    print(classification_report(labels,pred))  \n",
    "    print(accuracy_score(labels, pred))\n",
    "ans = model.predict(X_test)\n",
    "labels = [np.where(r==1)[0][0] for r in Y_test]\n",
    "ans = np.argmax(ans,axis=1)\n",
    "accuracy_score(ans,labels)\n",
    "results(ans,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "primary-cleaning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0  130   14 1127]]\n",
      "1/1 - 0s\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "# Predicting with CNN\n",
    "twt = [\"weather is fantastic.\"]\n",
    "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "twt = tokenizer.texts_to_sequences(twt)\n",
    "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n",
    "print(twt)\n",
    "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "if (np.argmax(sentiment) == 0):\n",
    "  print(\"negative\")\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "  print(\"neutral\")\n",
    "else:\n",
    "  print('positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "honey-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF Machine Learning (SVM) with Prediction\n",
    "data_2 = data[['text']]\n",
    "X = data_2['text']\n",
    "Y = data['airline_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "minor-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X, Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "competitive-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "moderate-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "public-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sudden-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "twelve-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('tfidf',TfidfVectorizer()),('clf',LinearSVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fatty-polymer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "demographic-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "incomplete-diesel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[848  54  22]\n",
      " [107 164  31]\n",
      " [ 39  32 167]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.92      0.88       924\n",
      "     neutral       0.66      0.54      0.59       302\n",
      "    positive       0.76      0.70      0.73       238\n",
      "\n",
      "    accuracy                           0.81      1464\n",
      "   macro avg       0.76      0.72      0.74      1464\n",
      "weighted avg       0.80      0.81      0.80      1464\n",
      "\n",
      "0.805327868852459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "latest-undergraduate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"Happy Valentine's day\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "clean-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "awful-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ordered-airline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                ('clf', RandomForestClassifier())])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('tfidf',TfidfVectorizer()),('clf',RandomForestClassifier())])\n",
    "text_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "interpreted-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "assigned-circle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[892  26   6]\n",
      " [176 107  19]\n",
      " [100  26 112]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.97      0.85       924\n",
      "     neutral       0.67      0.35      0.46       302\n",
      "    positive       0.82      0.47      0.60       238\n",
      "\n",
      "    accuracy                           0.76      1464\n",
      "   macro avg       0.75      0.60      0.64      1464\n",
      "weighted avg       0.75      0.76      0.73      1464\n",
      "\n",
      "0.7588797814207651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "heard-staff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"superb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "burning-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "#clf.fit(X_train_tfidf,y_train)\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('tfidf',TfidfVectorizer()),('clf',MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "framed-warehouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "designed-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "durable-listening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[923   1   0]\n",
      " [267  32   3]\n",
      " [204   3  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.66      1.00      0.80       924\n",
      "     neutral       0.89      0.11      0.19       302\n",
      "    positive       0.91      0.13      0.23       238\n",
      "\n",
      "    accuracy                           0.67      1464\n",
      "   macro avg       0.82      0.41      0.40      1464\n",
      "weighted avg       0.75      0.67      0.58      1464\n",
      "\n",
      "0.6734972677595629\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "exposed-highway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative'], dtype='<U8')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"superb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-apache",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
